{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlBmZ10SzD6AVGgcDZhVHr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameph/Icog_labs/blob/main/Simple_neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZpjTUC-Nw7m6",
        "outputId": "9837ae25-8389-4a4c-8449-f688bae4acd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 1.0985 | Optimizer: relu\n",
            "Epoch 50 | Loss: 1.1100 | Optimizer: relu\n",
            "Epoch 100 | Loss: 1.0900 | Optimizer: relu\n",
            "Epoch 150 | Loss: 1.0857 | Optimizer: relu\n",
            "Epoch 200 | Loss: 1.0830 | Optimizer: relu\n",
            "Epoch 250 | Loss: 1.1396 | Optimizer: relu\n",
            "Sample predictions: [0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# === Activation Functions ===\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# === Loss Function ===\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)] + 1e-9)\n",
        "    return np.sum(log_likelihood) / m\n",
        "\n",
        "# === Dense Layer with Optimizer Support ===\n",
        "class DenseLayer:\n",
        "    def __init__(self, input_size, output_size, activation='tanh'):\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.1\n",
        "        self.bias = np.zeros((1, output_size))\n",
        "\n",
        "        self.activation_name = activation\n",
        "        self.activation, self.activation_derivative = self.get_activation_fn(activation)\n",
        "\n",
        "        # For momentum\n",
        "        self.v_w = np.zeros_like(self.weights)\n",
        "        self.v_b = np.zeros_like(self.bias)\n",
        "\n",
        "        # For Adam\n",
        "        self.m_w = np.zeros_like(self.weights)\n",
        "        self.v_w_adam = np.zeros_like(self.weights)\n",
        "        self.m_b = np.zeros_like(self.bias)\n",
        "        self.v_b_adam = np.zeros_like(self.bias)\n",
        "\n",
        "        self.t = 0  # timestep for Adam\n",
        "\n",
        "    def get_activation_fn(self, name):\n",
        "        if name == 'relu':\n",
        "            return relu, relu_derivative\n",
        "        elif name == 'sigmoid':\n",
        "            return sigmoid, sigmoid_derivative\n",
        "        elif name == 'tanh':\n",
        "            return tanh, tanh_derivative\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation\")\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input = X\n",
        "        self.z = np.dot(X, self.weights) + self.bias\n",
        "        self.a = self.activation(self.z)\n",
        "        return self.a\n",
        "\n",
        "    def backward(self, grad_output, learning_rate, optimizer='gd', beta=0.9, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        grad_z = grad_output * self.activation_derivative(self.z)\n",
        "        grad_w = np.dot(self.input.T, grad_z) / self.input.shape[0]\n",
        "        grad_b = np.sum(grad_z, axis=0, keepdims=True) / self.input.shape[0]\n",
        "\n",
        "        if optimizer == 'momentum':\n",
        "            self.v_w = beta * self.v_w + (1 - beta) * grad_w\n",
        "            self.v_b = beta * self.v_b + (1 - beta) * grad_b\n",
        "            self.weights -= learning_rate * self.v_w\n",
        "            self.bias -= learning_rate * self.v_b\n",
        "\n",
        "        elif optimizer == 'adam':\n",
        "            self.t += 1\n",
        "            self.m_w = beta1 * self.m_w + (1 - beta1) * grad_w\n",
        "            self.v_w_adam = beta2 * self.v_w_adam + (1 - beta2) * (grad_w ** 2)\n",
        "\n",
        "            m_w_hat = self.m_w / (1 - beta1 ** self.t)\n",
        "            v_w_hat = self.v_w_adam / (1 - beta2 ** self.t)\n",
        "\n",
        "            self.weights -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
        "\n",
        "            self.m_b = beta1 * self.m_b + (1 - beta1) * grad_b\n",
        "            self.v_b_adam = beta2 * self.v_b_adam + (1 - beta2) * (grad_b ** 2)\n",
        "\n",
        "            m_b_hat = self.m_b / (1 - beta1 ** self.t)\n",
        "            v_b_hat = self.v_b_adam / (1 - beta2 ** self.t)\n",
        "\n",
        "            self.bias -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "        else:  # standard gradient descent\n",
        "            self.weights -= learning_rate * grad_w\n",
        "            self.bias -= learning_rate * grad_b\n",
        "\n",
        "        return np.dot(grad_z, self.weights.T)\n",
        "\n",
        "# === Neural Network ===\n",
        "class SimpleThreeLayerNN:\n",
        "    def __init__(self):\n",
        "        self.layer1 = DenseLayer(input_size=3, output_size=1, activation='relu')\n",
        "        self.layer2 = DenseLayer(input_size=1, output_size=1, activation='tanh')  # Example: tanh\n",
        "        self.layer3 = DenseLayer(input_size=1, output_size=3, activation='relu')  # Output layer before softmax\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.a1 = self.layer1.forward(X)\n",
        "        self.a2 = self.layer2.forward(self.a1)\n",
        "        self.z3 = self.layer3.forward(self.a2)\n",
        "        self.a3 = softmax(self.z3)\n",
        "        return self.a3\n",
        "\n",
        "    def backward(self, X, y_true, y_pred, learning_rate, optimizer):\n",
        "        m = y_true.shape[0]\n",
        "        grad_z3 = (y_pred - y_true) / m\n",
        "\n",
        "        grad_a2 = self.layer3.backward(grad_z3, learning_rate, optimizer)\n",
        "        grad_a1 = self.layer2.backward(grad_a2, learning_rate, optimizer)\n",
        "        _ = self.layer1.backward(grad_a1, learning_rate, optimizer)\n",
        "\n",
        "    def train(self, X, y, epochs=300, batch_size=16, learning_rate=0.01, optimizer='gd'):\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                X_batch = X_shuffled[i:i+batch_size]\n",
        "                y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                y_pred = self.forward(X_batch)\n",
        "                loss = cross_entropy_loss(y_batch, y_pred)\n",
        "                self.backward(X_batch, y_batch, y_pred, learning_rate, optimizer)\n",
        "\n",
        "            if epoch % 50 == 0:\n",
        "                print(f\"Epoch {epoch} | Loss: {loss:.4f} | Optimizer: {optimizer}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.forward(X)\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "# === Example Usage ===\n",
        "np.random.seed(42)\n",
        "\n",
        "X_train = np.random.randn(100, 3)\n",
        "y_train = np.zeros((100, 3))\n",
        "for i in range(100):\n",
        "    y_train[i, np.random.randint(0, 3)] = 1\n",
        "\n",
        "model = SimpleThreeLayerNN()\n",
        "model.train(X_train, y_train, epochs=300, batch_size=16, learning_rate=0.01, optimizer='relu')  # Try 'gd', 'momentum', 'adam'\n",
        "\n",
        "# Predict\n",
        "preds = model.predict(X_train)\n",
        "print(\"Sample predictions:\", preds[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "esKunSVuyH5k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}