{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYblmSPbVPIldLTOiBp+0+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameph/Icog_labs/blob/main/Simple_neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZpjTUC-Nw7m6",
        "outputId": "5fc46cb6-1b59-46fa-9244-0948fae68461",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Loss: 0.7142\n",
            "Epoch 2/500, Loss: 0.6522\n",
            "Epoch 3/500, Loss: 0.6813\n",
            "Epoch 4/500, Loss: 0.6982\n",
            "Epoch 5/500, Loss: 0.6895\n",
            "Epoch 6/500, Loss: 0.6622\n",
            "Epoch 7/500, Loss: 0.6204\n",
            "Epoch 8/500, Loss: 0.5700\n",
            "Epoch 9/500, Loss: 0.5111\n",
            "Epoch 10/500, Loss: 0.4663\n",
            "Epoch 11/500, Loss: 0.4407\n",
            "Epoch 12/500, Loss: 0.4167\n",
            "Epoch 13/500, Loss: 0.3773\n",
            "Epoch 14/500, Loss: 0.3198\n",
            "Epoch 15/500, Loss: 0.2604\n",
            "Epoch 16/500, Loss: 0.2086\n",
            "Epoch 17/500, Loss: 0.1846\n",
            "Epoch 18/500, Loss: 0.1615\n",
            "Epoch 19/500, Loss: 0.1402\n",
            "Epoch 20/500, Loss: 0.1212\n",
            "Epoch 21/500, Loss: 0.1046\n",
            "Epoch 22/500, Loss: 0.0904\n",
            "Epoch 23/500, Loss: 0.0782\n",
            "Epoch 24/500, Loss: 0.0680\n",
            "Epoch 25/500, Loss: 0.0592\n",
            "Epoch 26/500, Loss: 0.0518\n",
            "Epoch 27/500, Loss: 0.0455\n",
            "Epoch 28/500, Loss: 0.0402\n",
            "Epoch 29/500, Loss: 0.0356\n",
            "Epoch 30/500, Loss: 0.0318\n",
            "Epoch 31/500, Loss: 0.0284\n",
            "Epoch 32/500, Loss: 0.0256\n",
            "Epoch 33/500, Loss: 0.0231\n",
            "Epoch 34/500, Loss: 0.0210\n",
            "Epoch 35/500, Loss: 0.0192\n",
            "Epoch 36/500, Loss: 0.0176\n",
            "Epoch 37/500, Loss: 0.0162\n",
            "Epoch 38/500, Loss: 0.0150\n",
            "Epoch 39/500, Loss: 0.0139\n",
            "Epoch 40/500, Loss: 0.0130\n",
            "Epoch 41/500, Loss: 0.0122\n",
            "Epoch 42/500, Loss: 0.0114\n",
            "Epoch 43/500, Loss: 0.0108\n",
            "Epoch 44/500, Loss: 0.0102\n",
            "Epoch 45/500, Loss: 0.0097\n",
            "Epoch 46/500, Loss: 0.0092\n",
            "Epoch 47/500, Loss: 0.0088\n",
            "Epoch 48/500, Loss: 0.0084\n",
            "Epoch 49/500, Loss: 0.0080\n",
            "Epoch 50/500, Loss: 0.0077\n",
            "Epoch 51/500, Loss: 0.0074\n",
            "Epoch 52/500, Loss: 0.0071\n",
            "Epoch 53/500, Loss: 0.0069\n",
            "Epoch 54/500, Loss: 0.0067\n",
            "Epoch 55/500, Loss: 0.0065\n",
            "Epoch 56/500, Loss: 0.0063\n",
            "Epoch 57/500, Loss: 0.0061\n",
            "Epoch 58/500, Loss: 0.0059\n",
            "Epoch 59/500, Loss: 0.0058\n",
            "Epoch 60/500, Loss: 0.0056\n",
            "Epoch 61/500, Loss: 0.0055\n",
            "Epoch 62/500, Loss: 0.0054\n",
            "Epoch 63/500, Loss: 0.0052\n",
            "Epoch 64/500, Loss: 0.0051\n",
            "Epoch 65/500, Loss: 0.0050\n",
            "Epoch 66/500, Loss: 0.0049\n",
            "Epoch 67/500, Loss: 0.0048\n",
            "Epoch 68/500, Loss: 0.0047\n",
            "Epoch 69/500, Loss: 0.0047\n",
            "Epoch 70/500, Loss: 0.0046\n",
            "Epoch 71/500, Loss: 0.0045\n",
            "Epoch 72/500, Loss: 0.0044\n",
            "Epoch 73/500, Loss: 0.0043\n",
            "Epoch 74/500, Loss: 0.0043\n",
            "Epoch 75/500, Loss: 0.0042\n",
            "Epoch 76/500, Loss: 0.0041\n",
            "Epoch 77/500, Loss: 0.0041\n",
            "Epoch 78/500, Loss: 0.0040\n",
            "Epoch 79/500, Loss: 0.0040\n",
            "Epoch 80/500, Loss: 0.0039\n",
            "Epoch 81/500, Loss: 0.0039\n",
            "Epoch 82/500, Loss: 0.0038\n",
            "Epoch 83/500, Loss: 0.0038\n",
            "Epoch 84/500, Loss: 0.0037\n",
            "Epoch 85/500, Loss: 0.0037\n",
            "Epoch 86/500, Loss: 0.0036\n",
            "Epoch 87/500, Loss: 0.0036\n",
            "Epoch 88/500, Loss: 0.0035\n",
            "Epoch 89/500, Loss: 0.0035\n",
            "Epoch 90/500, Loss: 0.0034\n",
            "Epoch 91/500, Loss: 0.0034\n",
            "Epoch 92/500, Loss: 0.0034\n",
            "Epoch 93/500, Loss: 0.0033\n",
            "Epoch 94/500, Loss: 0.0033\n",
            "Epoch 95/500, Loss: 0.0032\n",
            "Epoch 96/500, Loss: 0.0032\n",
            "Epoch 97/500, Loss: 0.0032\n",
            "Epoch 98/500, Loss: 0.0031\n",
            "Epoch 99/500, Loss: 0.0031\n",
            "Epoch 100/500, Loss: 0.0031\n",
            "Epoch 101/500, Loss: 0.0030\n",
            "Epoch 102/500, Loss: 0.0030\n",
            "Epoch 103/500, Loss: 0.0030\n",
            "Epoch 104/500, Loss: 0.0029\n",
            "Epoch 105/500, Loss: 0.0029\n",
            "Epoch 106/500, Loss: 0.0029\n",
            "Epoch 107/500, Loss: 0.0028\n",
            "Epoch 108/500, Loss: 0.0028\n",
            "Epoch 109/500, Loss: 0.0028\n",
            "Epoch 110/500, Loss: 0.0028\n",
            "Epoch 111/500, Loss: 0.0027\n",
            "Epoch 112/500, Loss: 0.0027\n",
            "Epoch 113/500, Loss: 0.0027\n",
            "Epoch 114/500, Loss: 0.0027\n",
            "Epoch 115/500, Loss: 0.0026\n",
            "Epoch 116/500, Loss: 0.0026\n",
            "Epoch 117/500, Loss: 0.0026\n",
            "Epoch 118/500, Loss: 0.0026\n",
            "Epoch 119/500, Loss: 0.0025\n",
            "Epoch 120/500, Loss: 0.0025\n",
            "Epoch 121/500, Loss: 0.0025\n",
            "Epoch 122/500, Loss: 0.0025\n",
            "Epoch 123/500, Loss: 0.0024\n",
            "Epoch 124/500, Loss: 0.0024\n",
            "Epoch 125/500, Loss: 0.0024\n",
            "Epoch 126/500, Loss: 0.0024\n",
            "Epoch 127/500, Loss: 0.0023\n",
            "Epoch 128/500, Loss: 0.0023\n",
            "Epoch 129/500, Loss: 0.0023\n",
            "Epoch 130/500, Loss: 0.0023\n",
            "Epoch 131/500, Loss: 0.0023\n",
            "Epoch 132/500, Loss: 0.0022\n",
            "Epoch 133/500, Loss: 0.0022\n",
            "Epoch 134/500, Loss: 0.0022\n",
            "Epoch 135/500, Loss: 0.0022\n",
            "Epoch 136/500, Loss: 0.0022\n",
            "Epoch 137/500, Loss: 0.0022\n",
            "Epoch 138/500, Loss: 0.0021\n",
            "Epoch 139/500, Loss: 0.0021\n",
            "Epoch 140/500, Loss: 0.0021\n",
            "Epoch 141/500, Loss: 0.0021\n",
            "Epoch 142/500, Loss: 0.0021\n",
            "Epoch 143/500, Loss: 0.0020\n",
            "Epoch 144/500, Loss: 0.0020\n",
            "Epoch 145/500, Loss: 0.0020\n",
            "Epoch 146/500, Loss: 0.0020\n",
            "Epoch 147/500, Loss: 0.0020\n",
            "Epoch 148/500, Loss: 0.0020\n",
            "Epoch 149/500, Loss: 0.0019\n",
            "Epoch 150/500, Loss: 0.0019\n",
            "Epoch 151/500, Loss: 0.0019\n",
            "Epoch 152/500, Loss: 0.0019\n",
            "Epoch 153/500, Loss: 0.0019\n",
            "Epoch 154/500, Loss: 0.0019\n",
            "Epoch 155/500, Loss: 0.0019\n",
            "Epoch 156/500, Loss: 0.0018\n",
            "Epoch 157/500, Loss: 0.0018\n",
            "Epoch 158/500, Loss: 0.0018\n",
            "Epoch 159/500, Loss: 0.0018\n",
            "Epoch 160/500, Loss: 0.0018\n",
            "Epoch 161/500, Loss: 0.0018\n",
            "Epoch 162/500, Loss: 0.0018\n",
            "Epoch 163/500, Loss: 0.0017\n",
            "Epoch 164/500, Loss: 0.0017\n",
            "Epoch 165/500, Loss: 0.0017\n",
            "Epoch 166/500, Loss: 0.0017\n",
            "Epoch 167/500, Loss: 0.0017\n",
            "Epoch 168/500, Loss: 0.0017\n",
            "Epoch 169/500, Loss: 0.0017\n",
            "Epoch 170/500, Loss: 0.0017\n",
            "Epoch 171/500, Loss: 0.0016\n",
            "Epoch 172/500, Loss: 0.0016\n",
            "Epoch 173/500, Loss: 0.0016\n",
            "Epoch 174/500, Loss: 0.0016\n",
            "Epoch 175/500, Loss: 0.0016\n",
            "Epoch 176/500, Loss: 0.0016\n",
            "Epoch 177/500, Loss: 0.0016\n",
            "Epoch 178/500, Loss: 0.0016\n",
            "Epoch 179/500, Loss: 0.0016\n",
            "Epoch 180/500, Loss: 0.0015\n",
            "Epoch 181/500, Loss: 0.0015\n",
            "Epoch 182/500, Loss: 0.0015\n",
            "Epoch 183/500, Loss: 0.0015\n",
            "Epoch 184/500, Loss: 0.0015\n",
            "Epoch 185/500, Loss: 0.0015\n",
            "Epoch 186/500, Loss: 0.0015\n",
            "Epoch 187/500, Loss: 0.0015\n",
            "Epoch 188/500, Loss: 0.0015\n",
            "Epoch 189/500, Loss: 0.0015\n",
            "Epoch 190/500, Loss: 0.0014\n",
            "Epoch 191/500, Loss: 0.0014\n",
            "Epoch 192/500, Loss: 0.0014\n",
            "Epoch 193/500, Loss: 0.0014\n",
            "Epoch 194/500, Loss: 0.0014\n",
            "Epoch 195/500, Loss: 0.0014\n",
            "Epoch 196/500, Loss: 0.0014\n",
            "Epoch 197/500, Loss: 0.0014\n",
            "Epoch 198/500, Loss: 0.0014\n",
            "Epoch 199/500, Loss: 0.0014\n",
            "Epoch 200/500, Loss: 0.0014\n",
            "Epoch 201/500, Loss: 0.0013\n",
            "Epoch 202/500, Loss: 0.0013\n",
            "Epoch 203/500, Loss: 0.0013\n",
            "Epoch 204/500, Loss: 0.0013\n",
            "Epoch 205/500, Loss: 0.0013\n",
            "Epoch 206/500, Loss: 0.0013\n",
            "Epoch 207/500, Loss: 0.0013\n",
            "Epoch 208/500, Loss: 0.0013\n",
            "Epoch 209/500, Loss: 0.0013\n",
            "Epoch 210/500, Loss: 0.0013\n",
            "Epoch 211/500, Loss: 0.0013\n",
            "Epoch 212/500, Loss: 0.0013\n",
            "Epoch 213/500, Loss: 0.0012\n",
            "Epoch 214/500, Loss: 0.0012\n",
            "Epoch 215/500, Loss: 0.0012\n",
            "Epoch 216/500, Loss: 0.0012\n",
            "Epoch 217/500, Loss: 0.0012\n",
            "Epoch 218/500, Loss: 0.0012\n",
            "Epoch 219/500, Loss: 0.0012\n",
            "Epoch 220/500, Loss: 0.0012\n",
            "Epoch 221/500, Loss: 0.0012\n",
            "Epoch 222/500, Loss: 0.0012\n",
            "Epoch 223/500, Loss: 0.0012\n",
            "Epoch 224/500, Loss: 0.0012\n",
            "Epoch 225/500, Loss: 0.0012\n",
            "Epoch 226/500, Loss: 0.0012\n",
            "Epoch 227/500, Loss: 0.0011\n",
            "Epoch 228/500, Loss: 0.0011\n",
            "Epoch 229/500, Loss: 0.0011\n",
            "Epoch 230/500, Loss: 0.0011\n",
            "Epoch 231/500, Loss: 0.0011\n",
            "Epoch 232/500, Loss: 0.0011\n",
            "Epoch 233/500, Loss: 0.0011\n",
            "Epoch 234/500, Loss: 0.0011\n",
            "Epoch 235/500, Loss: 0.0011\n",
            "Epoch 236/500, Loss: 0.0011\n",
            "Epoch 237/500, Loss: 0.0011\n",
            "Epoch 238/500, Loss: 0.0011\n",
            "Epoch 239/500, Loss: 0.0011\n",
            "Epoch 240/500, Loss: 0.0011\n",
            "Epoch 241/500, Loss: 0.0011\n",
            "Epoch 242/500, Loss: 0.0011\n",
            "Epoch 243/500, Loss: 0.0010\n",
            "Epoch 244/500, Loss: 0.0010\n",
            "Epoch 245/500, Loss: 0.0010\n",
            "Epoch 246/500, Loss: 0.0010\n",
            "Epoch 247/500, Loss: 0.0010\n",
            "Epoch 248/500, Loss: 0.0010\n",
            "Epoch 249/500, Loss: 0.0010\n",
            "Epoch 250/500, Loss: 0.0010\n",
            "Epoch 251/500, Loss: 0.0010\n",
            "Epoch 252/500, Loss: 0.0010\n",
            "Epoch 253/500, Loss: 0.0010\n",
            "Epoch 254/500, Loss: 0.0010\n",
            "Epoch 255/500, Loss: 0.0010\n",
            "Epoch 256/500, Loss: 0.0010\n",
            "Epoch 257/500, Loss: 0.0010\n",
            "Epoch 258/500, Loss: 0.0010\n",
            "Epoch 259/500, Loss: 0.0010\n",
            "Epoch 260/500, Loss: 0.0010\n",
            "Epoch 261/500, Loss: 0.0010\n",
            "Epoch 262/500, Loss: 0.0009\n",
            "Epoch 263/500, Loss: 0.0009\n",
            "Epoch 264/500, Loss: 0.0009\n",
            "Epoch 265/500, Loss: 0.0009\n",
            "Epoch 266/500, Loss: 0.0009\n",
            "Epoch 267/500, Loss: 0.0009\n",
            "Epoch 268/500, Loss: 0.0009\n",
            "Epoch 269/500, Loss: 0.0009\n",
            "Epoch 270/500, Loss: 0.0009\n",
            "Epoch 271/500, Loss: 0.0009\n",
            "Epoch 272/500, Loss: 0.0009\n",
            "Epoch 273/500, Loss: 0.0009\n",
            "Epoch 274/500, Loss: 0.0009\n",
            "Epoch 275/500, Loss: 0.0009\n",
            "Epoch 276/500, Loss: 0.0009\n",
            "Epoch 277/500, Loss: 0.0009\n",
            "Epoch 278/500, Loss: 0.0009\n",
            "Epoch 279/500, Loss: 0.0009\n",
            "Epoch 280/500, Loss: 0.0009\n",
            "Epoch 281/500, Loss: 0.0009\n",
            "Epoch 282/500, Loss: 0.0009\n",
            "Epoch 283/500, Loss: 0.0009\n",
            "Epoch 284/500, Loss: 0.0009\n",
            "Epoch 285/500, Loss: 0.0008\n",
            "Epoch 286/500, Loss: 0.0008\n",
            "Epoch 287/500, Loss: 0.0008\n",
            "Epoch 288/500, Loss: 0.0008\n",
            "Epoch 289/500, Loss: 0.0008\n",
            "Epoch 290/500, Loss: 0.0008\n",
            "Epoch 291/500, Loss: 0.0008\n",
            "Epoch 292/500, Loss: 0.0008\n",
            "Epoch 293/500, Loss: 0.0008\n",
            "Epoch 294/500, Loss: 0.0008\n",
            "Epoch 295/500, Loss: 0.0008\n",
            "Epoch 296/500, Loss: 0.0008\n",
            "Epoch 297/500, Loss: 0.0008\n",
            "Epoch 298/500, Loss: 0.0008\n",
            "Epoch 299/500, Loss: 0.0008\n",
            "Epoch 300/500, Loss: 0.0008\n",
            "Epoch 301/500, Loss: 0.0008\n",
            "Epoch 302/500, Loss: 0.0008\n",
            "Epoch 303/500, Loss: 0.0008\n",
            "Epoch 304/500, Loss: 0.0008\n",
            "Epoch 305/500, Loss: 0.0008\n",
            "Epoch 306/500, Loss: 0.0008\n",
            "Epoch 307/500, Loss: 0.0008\n",
            "Epoch 308/500, Loss: 0.0008\n",
            "Epoch 309/500, Loss: 0.0008\n",
            "Epoch 310/500, Loss: 0.0008\n",
            "Epoch 311/500, Loss: 0.0008\n",
            "Epoch 312/500, Loss: 0.0007\n",
            "Epoch 313/500, Loss: 0.0007\n",
            "Epoch 314/500, Loss: 0.0007\n",
            "Epoch 315/500, Loss: 0.0007\n",
            "Epoch 316/500, Loss: 0.0007\n",
            "Epoch 317/500, Loss: 0.0007\n",
            "Epoch 318/500, Loss: 0.0007\n",
            "Epoch 319/500, Loss: 0.0007\n",
            "Epoch 320/500, Loss: 0.0007\n",
            "Epoch 321/500, Loss: 0.0007\n",
            "Epoch 322/500, Loss: 0.0007\n",
            "Epoch 323/500, Loss: 0.0007\n",
            "Epoch 324/500, Loss: 0.0007\n",
            "Epoch 325/500, Loss: 0.0007\n",
            "Epoch 326/500, Loss: 0.0007\n",
            "Epoch 327/500, Loss: 0.0007\n",
            "Epoch 328/500, Loss: 0.0007\n",
            "Epoch 329/500, Loss: 0.0007\n",
            "Epoch 330/500, Loss: 0.0007\n",
            "Epoch 331/500, Loss: 0.0007\n",
            "Epoch 332/500, Loss: 0.0007\n",
            "Epoch 333/500, Loss: 0.0007\n",
            "Epoch 334/500, Loss: 0.0007\n",
            "Epoch 335/500, Loss: 0.0007\n",
            "Epoch 336/500, Loss: 0.0007\n",
            "Epoch 337/500, Loss: 0.0007\n",
            "Epoch 338/500, Loss: 0.0007\n",
            "Epoch 339/500, Loss: 0.0007\n",
            "Epoch 340/500, Loss: 0.0007\n",
            "Epoch 341/500, Loss: 0.0007\n",
            "Epoch 342/500, Loss: 0.0007\n",
            "Epoch 343/500, Loss: 0.0007\n",
            "Epoch 344/500, Loss: 0.0007\n",
            "Epoch 345/500, Loss: 0.0006\n",
            "Epoch 346/500, Loss: 0.0006\n",
            "Epoch 347/500, Loss: 0.0006\n",
            "Epoch 348/500, Loss: 0.0006\n",
            "Epoch 349/500, Loss: 0.0006\n",
            "Epoch 350/500, Loss: 0.0006\n",
            "Epoch 351/500, Loss: 0.0006\n",
            "Epoch 352/500, Loss: 0.0006\n",
            "Epoch 353/500, Loss: 0.0006\n",
            "Epoch 354/500, Loss: 0.0006\n",
            "Epoch 355/500, Loss: 0.0006\n",
            "Epoch 356/500, Loss: 0.0006\n",
            "Epoch 357/500, Loss: 0.0006\n",
            "Epoch 358/500, Loss: 0.0006\n",
            "Epoch 359/500, Loss: 0.0006\n",
            "Epoch 360/500, Loss: 0.0006\n",
            "Epoch 361/500, Loss: 0.0006\n",
            "Epoch 362/500, Loss: 0.0006\n",
            "Epoch 363/500, Loss: 0.0006\n",
            "Epoch 364/500, Loss: 0.0006\n",
            "Epoch 365/500, Loss: 0.0006\n",
            "Epoch 366/500, Loss: 0.0006\n",
            "Epoch 367/500, Loss: 0.0006\n",
            "Epoch 368/500, Loss: 0.0006\n",
            "Epoch 369/500, Loss: 0.0006\n",
            "Epoch 370/500, Loss: 0.0006\n",
            "Epoch 371/500, Loss: 0.0006\n",
            "Epoch 372/500, Loss: 0.0006\n",
            "Epoch 373/500, Loss: 0.0006\n",
            "Epoch 374/500, Loss: 0.0006\n",
            "Epoch 375/500, Loss: 0.0006\n",
            "Epoch 376/500, Loss: 0.0006\n",
            "Epoch 377/500, Loss: 0.0006\n",
            "Epoch 378/500, Loss: 0.0006\n",
            "Epoch 379/500, Loss: 0.0006\n",
            "Epoch 380/500, Loss: 0.0006\n",
            "Epoch 381/500, Loss: 0.0006\n",
            "Epoch 382/500, Loss: 0.0006\n",
            "Epoch 383/500, Loss: 0.0006\n",
            "Epoch 384/500, Loss: 0.0006\n",
            "Epoch 385/500, Loss: 0.0006\n",
            "Epoch 386/500, Loss: 0.0006\n",
            "Epoch 387/500, Loss: 0.0006\n",
            "Epoch 388/500, Loss: 0.0005\n",
            "Epoch 389/500, Loss: 0.0005\n",
            "Epoch 390/500, Loss: 0.0005\n",
            "Epoch 391/500, Loss: 0.0005\n",
            "Epoch 392/500, Loss: 0.0005\n",
            "Epoch 393/500, Loss: 0.0005\n",
            "Epoch 394/500, Loss: 0.0005\n",
            "Epoch 395/500, Loss: 0.0005\n",
            "Epoch 396/500, Loss: 0.0005\n",
            "Epoch 397/500, Loss: 0.0005\n",
            "Epoch 398/500, Loss: 0.0005\n",
            "Epoch 399/500, Loss: 0.0005\n",
            "Epoch 400/500, Loss: 0.0005\n",
            "Epoch 401/500, Loss: 0.0005\n",
            "Epoch 402/500, Loss: 0.0005\n",
            "Epoch 403/500, Loss: 0.0005\n",
            "Epoch 404/500, Loss: 0.0005\n",
            "Epoch 405/500, Loss: 0.0005\n",
            "Epoch 406/500, Loss: 0.0005\n",
            "Epoch 407/500, Loss: 0.0005\n",
            "Epoch 408/500, Loss: 0.0005\n",
            "Epoch 409/500, Loss: 0.0005\n",
            "Epoch 410/500, Loss: 0.0005\n",
            "Epoch 411/500, Loss: 0.0005\n",
            "Epoch 412/500, Loss: 0.0005\n",
            "Epoch 413/500, Loss: 0.0005\n",
            "Epoch 414/500, Loss: 0.0005\n",
            "Epoch 415/500, Loss: 0.0005\n",
            "Epoch 416/500, Loss: 0.0005\n",
            "Epoch 417/500, Loss: 0.0005\n",
            "Epoch 418/500, Loss: 0.0005\n",
            "Epoch 419/500, Loss: 0.0005\n",
            "Epoch 420/500, Loss: 0.0005\n",
            "Epoch 421/500, Loss: 0.0005\n",
            "Epoch 422/500, Loss: 0.0005\n",
            "Epoch 423/500, Loss: 0.0005\n",
            "Epoch 424/500, Loss: 0.0005\n",
            "Epoch 425/500, Loss: 0.0005\n",
            "Epoch 426/500, Loss: 0.0005\n",
            "Epoch 427/500, Loss: 0.0005\n",
            "Epoch 428/500, Loss: 0.0005\n",
            "Epoch 429/500, Loss: 0.0005\n",
            "Epoch 430/500, Loss: 0.0005\n",
            "Epoch 431/500, Loss: 0.0005\n",
            "Epoch 432/500, Loss: 0.0005\n",
            "Epoch 433/500, Loss: 0.0005\n",
            "Epoch 434/500, Loss: 0.0005\n",
            "Epoch 435/500, Loss: 0.0005\n",
            "Epoch 436/500, Loss: 0.0005\n",
            "Epoch 437/500, Loss: 0.0005\n",
            "Epoch 438/500, Loss: 0.0005\n",
            "Epoch 439/500, Loss: 0.0005\n",
            "Epoch 440/500, Loss: 0.0005\n",
            "Epoch 441/500, Loss: 0.0005\n",
            "Epoch 442/500, Loss: 0.0005\n",
            "Epoch 443/500, Loss: 0.0005\n",
            "Epoch 444/500, Loss: 0.0005\n",
            "Epoch 445/500, Loss: 0.0005\n",
            "Epoch 446/500, Loss: 0.0004\n",
            "Epoch 447/500, Loss: 0.0004\n",
            "Epoch 448/500, Loss: 0.0004\n",
            "Epoch 449/500, Loss: 0.0004\n",
            "Epoch 450/500, Loss: 0.0004\n",
            "Epoch 451/500, Loss: 0.0004\n",
            "Epoch 452/500, Loss: 0.0004\n",
            "Epoch 453/500, Loss: 0.0004\n",
            "Epoch 454/500, Loss: 0.0004\n",
            "Epoch 455/500, Loss: 0.0004\n",
            "Epoch 456/500, Loss: 0.0004\n",
            "Epoch 457/500, Loss: 0.0004\n",
            "Epoch 458/500, Loss: 0.0004\n",
            "Epoch 459/500, Loss: 0.0004\n",
            "Epoch 460/500, Loss: 0.0004\n",
            "Epoch 461/500, Loss: 0.0004\n",
            "Epoch 462/500, Loss: 0.0004\n",
            "Epoch 463/500, Loss: 0.0004\n",
            "Epoch 464/500, Loss: 0.0004\n",
            "Epoch 465/500, Loss: 0.0004\n",
            "Epoch 466/500, Loss: 0.0004\n",
            "Epoch 467/500, Loss: 0.0004\n",
            "Epoch 468/500, Loss: 0.0004\n",
            "Epoch 469/500, Loss: 0.0004\n",
            "Epoch 470/500, Loss: 0.0004\n",
            "Epoch 471/500, Loss: 0.0004\n",
            "Epoch 472/500, Loss: 0.0004\n",
            "Epoch 473/500, Loss: 0.0004\n",
            "Epoch 474/500, Loss: 0.0004\n",
            "Epoch 475/500, Loss: 0.0004\n",
            "Epoch 476/500, Loss: 0.0004\n",
            "Epoch 477/500, Loss: 0.0004\n",
            "Epoch 478/500, Loss: 0.0004\n",
            "Epoch 479/500, Loss: 0.0004\n",
            "Epoch 480/500, Loss: 0.0004\n",
            "Epoch 481/500, Loss: 0.0004\n",
            "Epoch 482/500, Loss: 0.0004\n",
            "Epoch 483/500, Loss: 0.0004\n",
            "Epoch 484/500, Loss: 0.0004\n",
            "Epoch 485/500, Loss: 0.0004\n",
            "Epoch 486/500, Loss: 0.0004\n",
            "Epoch 487/500, Loss: 0.0004\n",
            "Epoch 488/500, Loss: 0.0004\n",
            "Epoch 489/500, Loss: 0.0004\n",
            "Epoch 490/500, Loss: 0.0004\n",
            "Epoch 491/500, Loss: 0.0004\n",
            "Epoch 492/500, Loss: 0.0004\n",
            "Epoch 493/500, Loss: 0.0004\n",
            "Epoch 494/500, Loss: 0.0004\n",
            "Epoch 495/500, Loss: 0.0004\n",
            "Epoch 496/500, Loss: 0.0004\n",
            "Epoch 497/500, Loss: 0.0004\n",
            "Epoch 498/500, Loss: 0.0004\n",
            "Epoch 499/500, Loss: 0.0004\n",
            "Epoch 500/500, Loss: 0.0004\n",
            "\n",
            "Predictions:\n",
            "[[9.99994866e-01 5.13402925e-06]\n",
            " [7.52813510e-04 9.99247186e-01]\n",
            " [9.99992529e-01 7.47089567e-06]\n",
            " [7.52813510e-04 9.99247186e-01]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return x > 0\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "# Loss function\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-9), axis=1))\n",
        "\n",
        "# Forward propagation\n",
        "def forward_propagation(X, weights, biases, activation):\n",
        "    if activation == \"relu\":\n",
        "        act_function, act_derivative = relu, relu_derivative\n",
        "    elif activation == \"sigmoid\":\n",
        "        act_function, act_derivative = sigmoid, sigmoid_derivative\n",
        "    elif activation == \"tanh\":\n",
        "        act_function, act_derivative = tanh, tanh_derivative\n",
        "\n",
        "    layer1 = act_function(np.dot(X, weights[0]) + biases[0])\n",
        "    layer2 = act_function(np.dot(layer1, weights[1]) + biases[1])\n",
        "    output = softmax(np.dot(layer2, weights[2]) + biases[2])\n",
        "    return layer1, layer2, output\n",
        "\n",
        "# Backpropagation\n",
        "def backpropagation(X, y_true, layer1, layer2, output, weights, activation):\n",
        "    if activation == \"relu\":\n",
        "        act_derivative = relu_derivative\n",
        "    elif activation == \"sigmoid\":\n",
        "        act_derivative = sigmoid_derivative\n",
        "    elif activation == \"tanh\":\n",
        "        act_derivative = tanh_derivative\n",
        "\n",
        "    m = X.shape[0]\n",
        "\n",
        "    d_output = output - y_true\n",
        "    d_layer2 = np.dot(d_output, weights[2].T) * act_derivative(layer2)\n",
        "    d_layer1 = np.dot(d_layer2, weights[1].T) * act_derivative(layer1)\n",
        "\n",
        "    return d_output, d_layer2, d_layer1\n",
        "# Gradient descent optimization\n",
        "def gradient_descent(weights, biases, gradients, learning_rate):\n",
        "    for i in range(len(weights)):\n",
        "        weights[i] -= learning_rate * gradients[f'dW{i+1}']\n",
        "        biases[i] -= learning_rate * gradients[f'dB{i+1}']\n",
        "\n",
        "# Adam optimizer implementation\n",
        "class AdamOptimizer:\n",
        "    def __init__(self, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, weights, biases, gradients):\n",
        "        self.t += 1\n",
        "        for i in range(len(weights)):\n",
        "            if f'dW{i+1}' not in self.m:\n",
        "                self.m[f'dW{i+1}'], self.v[f'dW{i+1}'] = np.zeros_like(gradients[f'dW{i+1}']), np.zeros_like(gradients[f'dW{i+1}'])\n",
        "                self.m[f'dB{i+1}'], self.v[f'dB{i+1}'] = np.zeros_like(gradients[f'dB{i+1}']), np.zeros_like(gradients[f'dB{i+1}'])\n",
        "\n",
        "            # Update biased first moment estimates\n",
        "            self.m[f'dW{i+1}'] = self.beta1 * self.m[f'dW{i+1}'] + (1 - self.beta1) * gradients[f'dW{i+1}']\n",
        "            self.m[f'dB{i+1}'] = self.beta1 * self.m[f'dB{i+1}'] + (1 - self.beta1) * gradients[f'dB{i+1}']\n",
        "\n",
        "            # Update biased second raw moment estimates\n",
        "            self.v[f'dW{i+1}'] = self.beta2 * self.v[f'dW{i+1}'] + (1 - self.beta2) * (gradients[f'dW{i+1}']**2)\n",
        "            self.v[f'dB{i+1}'] = self.beta2 * self.v[f'dB{i+1}'] + (1 - self.beta2) * (gradients[f'dB{i+1}']**2)\n",
        "\n",
        "            # Compute bias-corrected first and second moments\n",
        "            m_hat_dW = self.m[f'dW{i+1}'] / (1 - self.beta1**self.t)\n",
        "            v_hat_dW = self.v[f'dW{i+1}'] / (1 - self.beta2**self.t)\n",
        "            m_hat_dB = self.m[f'dB{i+1}'] / (1 - self.beta1**self.t)\n",
        "            v_hat_dB = self.v[f'dB{i+1}'] / (1 - self.beta2**self.t)\n",
        "\n",
        "            # Update parameters\n",
        "            weights[i] -= self.learning_rate * m_hat_dW / (np.sqrt(v_hat_dW) + self.epsilon)\n",
        "            biases[i] -= self.learning_rate * m_hat_dB / (np.sqrt(v_hat_dB) + self.epsilon)\n",
        "\n",
        "# Neural network training\n",
        "def train(X, y, epochs=100, batch_size=32, learning_rate=0.01, activation=\"relu\", optimizer=\"adam\"):\n",
        "    np.random.seed(42)\n",
        "    input_dim = X.shape[1]\n",
        "    hidden_dim = 3  # Small layers for simplicity\n",
        "    output_dim = y.shape[1]\n",
        "\n",
        "    weights = [np.random.randn(input_dim, hidden_dim),\n",
        "               np.random.randn(hidden_dim, hidden_dim),\n",
        "               np.random.randn(hidden_dim, output_dim)]\n",
        "    biases = [np.zeros((1, hidden_dim)),\n",
        "              np.zeros((1, hidden_dim)),\n",
        "              np.zeros((1, output_dim))]\n",
        "\n",
        "    # Initialize optimizer\n",
        "    if optimizer == \"adam\":\n",
        "        opt = AdamOptimizer(learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(X.shape[0])\n",
        "        X, y = X[indices], y[indices]\n",
        "\n",
        "        for i in range(0, X.shape[0], batch_size):\n",
        "            X_batch = X[i:i+batch_size]\n",
        "            y_batch = y[i:i+batch_size]\n",
        "\n",
        "            layer1, layer2, output = forward_propagation(X_batch, weights, biases, activation)\n",
        "            d_output, d_layer2, d_layer1 = backpropagation(X_batch, y_batch, layer1, layer2, output, weights, activation)\n",
        "\n",
        "            gradients = {\n",
        "                'dW1': np.dot(X_batch.T, d_layer1) / batch_size,\n",
        "                'dB1': np.sum(d_layer1, axis=0, keepdims=True) / batch_size,\n",
        "                'dW2': np.dot(layer1.T, d_layer2) / batch_size,\n",
        "                'dB2': np.sum(d_layer2, axis=0, keepdims=True) / batch_size,\n",
        "                'dW3': np.dot(layer2.T, d_output) / batch_size,\n",
        "                'dB3': np.sum(d_output, axis=0, keepdims=True) / batch_size,\n",
        "            }\n",
        "\n",
        "            if optimizer == \"gradient_descent\":\n",
        "                gradient_descent(weights, biases, gradients, learning_rate)\n",
        "            elif optimizer == \"adam\":\n",
        "                opt.update(weights, biases, gradients)\n",
        "\n",
        "        _, _, output = forward_propagation(X, weights, biases, activation)\n",
        "        loss = cross_entropy_loss(y, output)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Toy dataset\n",
        "X = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])  # Inputs (AND gate)\n",
        "y = np.array([[1, 0], [0, 1], [1, 0], [0, 1]])  # Outputs (One-hot encoded)\n",
        "\n",
        "# Train the model\n",
        "trained_weights, trained_biases = train(X, y, epochs=500, batch_size=4, learning_rate=0.1, activation=\"relu\", optimizer=\"adam\")\n",
        "\n",
        "# Test the model\n",
        "_, _, predictions = forward_propagation(X, trained_weights, trained_biases, activation=\"relu\")\n",
        "print(\"\\nPredictions:\")\n",
        "print(predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "esKunSVuyH5k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}